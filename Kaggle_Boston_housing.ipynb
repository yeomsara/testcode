{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribute Information:\n",
    "#### 1)  CRIM    - 자치시별 1인당 범죄율\n",
    "#### 2)  ZN      - 25000평방피트를 초과하는 거주지역의 비율\n",
    "#### 3)  INDUS   - 비소매상업지역이 점유하고 있는 토지의 비율\n",
    "#### 4)  CHAS    - 찰스강에 대한 더미변수(강의 경계에 위치한 경우는 1,아니면 0)\n",
    "#### 5)  NOX     - 10ppm당 농축 일산화질소\n",
    "#### 6)  RM      - 주택 1가구당 평균 방의개수\n",
    "#### 7)  AGE     - 1940년 이전에 건축된 소유주택의 비율\n",
    "#### 8)  DIS     - 5개의 보스턴 직업센터까지의 접근성 지수\n",
    "#### 9)  RAD     - 방사형 도로까지의 접근성 지수\n",
    "#### 10) TAX     - 10,000달러 당 재산세율\n",
    "#### 11) PTRATIO - 자치시(town)별 학생/교사 비율\n",
    "#### 12) B       - 1000(Bk - 0.63)^2 , bk는 자치시의 흑인의 비율을 뜻함\n",
    "#### 13) LSTAT   - 모집단의 하위계층의 비율(%)\n",
    "#### 14) MEDV    - 본인소유의 주택가격(중앙값) (단위:$1.000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 데이터 로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n",
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
      "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
      "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
      "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
      "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
      "\n",
      "   PTRATIO       B  LSTAT  MEDV  \n",
      "0     15.3  396.90   4.98  24.0  \n",
      "1     17.8  396.90   9.14  21.6  \n",
      "2     17.8  392.83   4.03  34.7  \n",
      "3     18.7  394.63   2.94  33.4  \n",
      "4     18.7  396.90   5.33  36.2  \n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "load_boston = load_boston()\n",
    "\n",
    "dfX = pd.DataFrame(load_boston.data, columns=load_boston.feature_names)\n",
    "dfy = pd.DataFrame(load_boston.target, columns=[\"MEDV\"])\n",
    "boston = pd.concat([dfX, dfy], axis=1)\n",
    "\n",
    "print(boston.shape)\n",
    "print(boston.head())\n",
    "# x_train, x_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 탐색 및 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 탐색에서은 아래 프로세스로 데이터를 탐색 후 전처리 한다\n",
    "\n",
    "#### 1. 데이터의 분포 확인 ( 기초 통계량 - MIN, MAX, QUANTILE, 결측치) \n",
    "#### 2. SKWED 데이터 셋 추출 및 리스케일링\n",
    "#### 3. 이상치 데이터 감지 및 변환 작업\n",
    "####    - 이상치 데이터는 이상치의 데이터가 굉장히 적을경우 (1% 미만)는 그냥 삭제 하거나\n",
    "####    - 다른값으로 대치(Imputation)\n",
    "####    - 치우쳐진 경우 스케일링작업(log,제곱근)\n",
    "####    - Min,max,Z-score등으로 정규화시킨다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) 데이터 탐색작업단계에서 ('ZN', 'B', 'CRIM') 세 변수가 skewed 한 분포를 보임과 동시에 이상치 데이터가 감지되는 데이터로\n",
    "#####     파악 할 수 있음\n",
    "##### (2) (1)의 변수는 아래 Feature선정시 해당 데이터가 종속변수와의 연관성이 있는지를 파악한 후, 제외할지 스케일링하여 input데이터로 사용할지\n",
    "#####     결정 한다.\n",
    "##### (3) 종속변수인 'MEDV'변수도 마찬가지로 이상치 데이터가 감지되었으며 총 506개의 데이터 중  40개(7.91%) 정도\n",
    "#####     종속변수의 이상치데이터가 너무 많을시 모델링의 결과가 왜곡될 수 있으며, 40개의 데이터를 모두 제거하기엔 데이터의 수가 줄어들고\n",
    "#####     과적합 현상이 생길 수 있음 MAX/MIN 값의 데이터만 절사 하는 수준으로 이상치 데이터를 제거 후 인풋데이터정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================\n",
      "데이터 기초 통계량  \n",
      " \n",
      "\n",
      "              CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
      "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
      "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
      "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
      "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
      "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
      "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
      "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
      "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
      "\n",
      "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
      "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
      "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
      "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
      "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
      "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
      "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
      "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
      "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
      "\n",
      "            LSTAT        MEDV  \n",
      "count  506.000000  506.000000  \n",
      "mean    12.653063   22.532806  \n",
      "std      7.141062    9.197104  \n",
      "min      1.730000    5.000000  \n",
      "25%      6.950000   17.025000  \n",
      "50%     11.360000   21.200000  \n",
      "75%     16.955000   25.000000  \n",
      "max     37.970000   50.000000  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    float64\n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    float64\n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    506 non-null    float64\n",
      " 13  MEDV     506 non-null    float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 55.5 KB\n",
      "\n",
      " 데이터  \n",
      " None\n",
      "===================================================================\n"
     ]
    }
   ],
   "source": [
    "# 데이터 기초 통계량 , 타입, 결측치 확인 \n",
    "# 결측치 없음\n",
    "# 데이터 타입은 모두 수치형 데이터\n",
    "# CHAS 데이터는 더미변수로 히스토그램,박스플랏을 보는 EDA에서는 제외\n",
    "print('===================================================================')\n",
    "print('데이터 기초 통계량  \\n \\n\\n', boston.describe())\n",
    "print('\\n 데이터  \\n',boston.info())\n",
    "# print('\\n 결측값 확인 \\n',boston.isnull().sum())\n",
    "print('===================================================================')\n",
    "\n",
    "# 데이터 분포 확인\n",
    "# CRIM , ZN, B 변수의 경우 Skewed한 분포를 보이고 있으며\n",
    "# CHAS 의 경우 바이너리 데이터로 대다수의 데이터가 0으로 포진되어 있다.\n",
    "\n",
    "def EDA_distributions(df):    \n",
    "    # except Binary data(chas)\n",
    "    for feature in df.columns:\n",
    "        print(feature)\n",
    "        if feature == 'CHAS':\n",
    "            print(Counter(df[feature]))\n",
    "            print(' ')\n",
    "        else :\n",
    "            plt.figure(figsize=(15,6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            fig = sns.boxplot(y=df[feature])\n",
    "            fig.set_title('')\n",
    "            fig.set_ylabel(feature)\n",
    "            \n",
    "            #histplot + kdeplot= distplot\n",
    "            plt.subplot(1, 2, 2)\n",
    "            fig = sns.distplot(df[feature])\n",
    "            fig.set_xlabel(feature)\n",
    "            plt.show()    \n",
    "        \n",
    "        \n",
    "# 이상치 탐색(iQR)\n",
    "def detect_outliers(df):\n",
    "    outlier_cols = []\n",
    "    perc_list = []\n",
    "    upper_limit = []\n",
    "    lower_limit = []\n",
    "    for col, data in df.items():\n",
    "        q1 = data.quantile(0.25)\n",
    "        q3 = data.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        scale = 1.5\n",
    "        lower = q1 - scale * iqr\n",
    "        upper = q3 + scale * iqr\n",
    "        v_col = data[(data < lower) | (data > upper)]\n",
    "        perc =( np.shape(v_col)[0] / np.shape(boston)[0] )* 100.0\n",
    "        print(\"Column %s outliers [ %s / %s ] = %.2f%%\" % (col,np.shape(v_col)[0],np.shape(df)[0], perc))\n",
    "        outlier_indices = np.shape(v_col)[0]\n",
    "        if outlier_indices == 0:\n",
    "            print('%s 변수에 탐지된 이상치는 없습니다.\\n'%col)\n",
    "        else:\n",
    "            print('%s 변수에 탐지된 이상치 데이터의 개수  : %s\\n'%(col,outlier_indices))\n",
    "            outlier_cols.append(col)\n",
    "            perc_list.append(perc)\n",
    "            lower_limit.append(lower)\n",
    "            upper_limit.append(upper)\n",
    "    result = pd.DataFrame({'outlier_list' : outlier_cols,\n",
    "                           'perc' : perc_list,\n",
    "                           'upper_limit':upper_limit,\n",
    "                           'lower_limit':lower_limit})\n",
    "    \n",
    "    return result.sort_values(by=['perc'],ascending = False)\n",
    "\n",
    "\n",
    "def check_skewed(df):\n",
    "    skew_df =  df.drop(['MEDV'], axis=1).skew()\n",
    "    sked_df = abs(skew_df).sort_values(ascending=False)\n",
    "    return sked_df[sked_df >= 2]\n",
    "\n",
    "# Data Scaling\n",
    "def log_scaling(df):\n",
    "    input_df = df\n",
    "    sked_list = ['CRIM', 'ZN','B']\n",
    "    for col in sked_list:\n",
    "        print(col)\n",
    "        print('before : ',df[col].skew())\n",
    "#         after_boxcox = stats.boxcox(input_df[col]+1)[0]\n",
    "        aa = np.log(input_df[col])\n",
    "        input_df[col] = pd.Series(aa)\n",
    "        print('after : ',input_df[col].skew())\n",
    "#         plt.subplot(1, 2, 2)\n",
    "        fig = sns.distplot(input_df[col])\n",
    "        fig.set_xlabel(col)\n",
    "        plt.show() \n",
    "\n",
    "def  std_scaling(df):\n",
    "    X = df.drop(['MEDV'], axis=1)\n",
    "    y = df['MEDV']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    cancer_scale = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    return cancer_scale\n",
    "        \n",
    "#  데이터 분포 체크\n",
    "EDA_distributions(boston)        \n",
    "#  이상치 점검 및 이상치가 있는 데이터 감지\n",
    "outlier_df = detect_outliers(boston.iloc[:,~boston.columns.isin(['CHAS'])])\n",
    "print('[ outlier_list ]\\n\\n%s'%outlier_df)\n",
    "#  SKED 데이터셋 추출 ( 왜도의 기준은 2이상으로 정의함 )\n",
    "sked_list = check_skewed(boston).index.to_list()\n",
    "print('=============================================================================================')\n",
    "print('\\noutlier 감지 리스트 : ', outlier_df['outlier_list'].to_list())\n",
    "print('\\nSKED한 데이터 리스트 : ', sked_list)\n",
    "print('Skewed하고 이상치가 있는 데이터 : ',set(sked_list)&set(outlier_df['outlier_list'].to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 종속변수 이상치 제거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = boston[(boston['MEDV'] > min(boston['MEDV']))&(boston['MEDV'] < max(boston['MEDV']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature 의 선정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection에는 총 3가지 방법론을 적용하여 3가지 방법론의 교집합을 독립변수로 셀렉함.\n",
    "\n",
    "### 1. 상관관계분석\n",
    "#### --> 상관관계 분석시에는 상관계수의 절대값이 0.3이상인 약한 상관관계이상의 변수들만 셀렉한 후 \n",
    "#### --> 다중공선성을 고려하여 각 독립변수간의 상관계수가 0.9이상인 데이터는 종속변수와의 상관계수가 높은 변수만 남기고 제거한다.\n",
    "### 2. Feature Importance\n",
    "#### --> Tree기반의 Feature Importance를 기준으로 시각화하면 importances rank가 0.01이상은 변수들만 셀렉\n",
    "#### --> 해당 방법론 적용시 위의 아웃라이어&SKEWED 분포를 가진 변수들은 제외됨\n",
    "### 3. 추정모델에 따른 feature의 weight를 기준으로 자동으로 selection해주는 selectfrommodel\n",
    "#### --> threshold(임계값)을 변수 선택에 사용하여 중요도가 크거나 같은 변수는 셀렉하고 중요도가 낮은 변수는 제외시키는 로직"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    ## 상관관계가 0.3 이상인 강한 상관관계를 보이는 변수만 선택\n",
    "    corr_matrix = boston.corr()\n",
    "    top_corr_feature = corr_matrix.index[abs(corr_matrix['MEDV']) >= 0.3].to_list()\n",
    "    except_cols = set(boston) - set(top_corr_feature)\n",
    "    \n",
    "    rank_corr_feature = abs(corr_matrix.loc[abs(corr_matrix['MEDV']) >= 0.3,['MEDV','CRIM']])\n",
    "    rank_corr_feature = rank_corr_feature.sort_values(by=['MEDV'],ascending=False)\n",
    "    \n",
    "    print('MEDV(Y)와 상관계수가 0.3 이하인 약한 상관관계를 보이는 변수 : %s'%except_cols )\n",
    "    plt.figure(figsize=(13,10))\n",
    "    sns.heatmap(boston[top_corr_feature].corr(), annot = True)\n",
    "    plt.show()\n",
    "    \n",
    "    # 다중공선성 ( 변수 선택 및 제거 )\n",
    "    # 독립변수간의 상관관계가 강한 경우 모델의 과적합 현상이 발생할 수 있음\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    # Find features with correlation greater than 0.9\n",
    "    feature = [column for column in upper.columns if any(upper[column] >= 0.9)]\n",
    "    ss = corr_matrix.TAX[corr_matrix.TAX >= 0.9].index.tolist()\n",
    "    print('다중공선성이 보이는 변수  : %s '%(ss))\n",
    "    final_feature = rank_corr_feature.drop(index='RAD', columns='MEDV').index.to_list()\n",
    "    return [col for col in final_feature if col != 'MEDV']\n",
    "    \n",
    "    \n",
    "def selectfrommodel(df):\n",
    "    X = df.drop(['MEDV'], axis=1)\n",
    "    y = df['MEDV']\n",
    "    input_cols = X.columns\n",
    "    lgb_selector   = SelectFromModel(estimator=LGBMRegressor()).fit(X, y)\n",
    "    xgb_selector   = SelectFromModel(estimator=XGBRegressor()).fit(X, y)\n",
    "    ridge_selector = SelectFromModel(estimator=Ridge()).fit(X, y)\n",
    "    lasso_selector = SelectFromModel(estimator=Lasso()).fit(X, y)\n",
    "    \n",
    "    feature_list = list(input_cols[lgb_selector.get_support()])+\\\n",
    "                   list(input_cols[xgb_selector.get_support()])+\\\n",
    "                   list(input_cols[ridge_selector.get_support()])+\\\n",
    "                   list(input_cols[lasso_selector.get_support()])\n",
    "    \n",
    "#     print('LightGBM : ',input_cols[lgb_selector.get_support()])\n",
    "#     print('xgboost  : ',input_cols[xgb_selector.get_support()])\n",
    "#     print('Ridge : '   ,input_cols[ridge_selector.get_support()])\n",
    "#     print('Lasso : '   ,input_cols[lasso_selector.get_support()])\n",
    "    \n",
    "    return list(set(feature_list))\n",
    "\n",
    "\n",
    "#Meta-transformer for selecting features based on importance weights.\n",
    "\n",
    "def Feature_Importance(df):\n",
    "    X = df.iloc[:,~df.columns.isin(['MEDV'])]\n",
    "    y = df['MEDV']\n",
    "    # RandomForest\n",
    "    clf = RandomForestRegressor(random_state=42, max_depth=6)\n",
    "    clf.fit(X, y)\n",
    "    feature_importance = clf.feature_importances_\n",
    "\n",
    "    # plot\n",
    "    df_fi = pd.DataFrame({'columns':X.columns, 'importances':feature_importance})\n",
    "    df_fi = df_fi[df_fi['importances'] > 0] # importance가 0이상인 것만 \n",
    "    df_fi = df_fi.sort_values(by=['importances'], ascending=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(15,7))\n",
    "    ax = sns.barplot(df_fi['columns'], df_fi['importances'])\n",
    "    ax.set_xticklabels(df_fi['columns'], rotation=80, fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return  df_fi[df_fi['importances'] >= 0.01]\n",
    "    \n",
    "corr_feature = correlation_matrix(input_df)\n",
    "print('다중공선성 제외 상관관계 변수 : ',corr_feature)\n",
    "sfm_list = selectfrommodel(input_df)\n",
    "print('\\nSelectfromModel 변수 : ',sfm_list )\n",
    "df_fi = Feature_Importance(boston)\n",
    "importance_feature = df_fi['columns'].to_list()\n",
    "print('\\nFeature Importance 변수 : ',importance_feature)\n",
    "\n",
    "intersection_feature = list(set(corr_feature)&set(sfm_list)&set(importance_feature))+['MEDV']\n",
    "print('\\n교집합 변수 : ',intersection_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 예측 모형 학습 및 성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3이하변수 RM\n",
      "X columns :  ['TAX', 'PTRATIO', 'RM', 'AGE', 'CRIM', 'LSTAT', 'NOX']\n",
      "original X_train : 354 /original X_test : 152 : \n",
      "original Y_train : 354 /original  Y_test : 152 : \n",
      "X_train : 341 / X_test : 147 : \n",
      "Y_train : 341 / Y_test : 147 : \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from IPython.core.display import Image\n",
    "# import numpy as np\n",
    "\n",
    "# from sklearn import svm\n",
    "\n",
    "# train,test 셋 분할\n",
    "def split_train_test(df,size):\n",
    "    X = df.drop(['MEDV'],axis=1)\n",
    "    Y = df['MEDV']\n",
    "    # random state = seed for shuffle\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=size, random_state=123)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def evaludation(y_true,y_pred):\n",
    "    mse = mean_squared_error(y_true,y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = (np.sqrt(mse))\n",
    "    return r2,rmse,mse\n",
    "\n",
    "final_df = input_df[intersection_feature]\n",
    "\n",
    "# select feature scaling if skew() > 0.3 \n",
    "for col in final_df.columns:\n",
    "    if np.abs(final_df[col].skew()) > 0.3:\n",
    "        final_df[col] = np.log1p(final_df[col])\n",
    "    else : \n",
    "        print('0.3이하변수 %s'%col)\n",
    "#     plt.figure(figsize=(15,10))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     fig = sns.distplot(input_df[col])\n",
    "#     fig.set_xlabel(col)\n",
    "#     plt.show()   \n",
    "\n",
    "#     plt.subplot(2, 2, 2)\n",
    "#     fig = sns.distplot(final_df[col])\n",
    "#     fig.set_xlabel(col)\n",
    "#     plt.show()    \n",
    "ori_X_train, ori_X_test, ori_Y_train, ori_Y_test = split_train_test(boston,0.3)\n",
    "X_train, X_test, Y_train, Y_test = split_train_test(final_df,0.3)\n",
    "\n",
    "print('X columns : ',X_train.columns.to_list())\n",
    "\n",
    "print('original X_train : %s /original X_test : %s : '%(len(ori_X_train),len(ori_X_test)))\n",
    "print('original Y_train : %s /original  Y_test : %s : '%(len(ori_Y_train),len(ori_Y_test)))\n",
    "\n",
    "print('X_train : %s / X_test : %s : '%(len(X_train),len(X_test)))\n",
    "print('Y_train : %s / Y_test : %s : '%(len(Y_train),len(Y_test)))\n",
    "\n",
    "\n",
    "def clf_grid(cat,X_train,X_test,Y_train,Y_test):\n",
    "    clf_list    = []\n",
    "    cat_list    = []\n",
    "    feature_len = []\n",
    "    feature     = []\n",
    "    x_train_len = []\n",
    "    x_test_len  = []\n",
    "    y_train_len = []\n",
    "    y_test_len  = []\n",
    "    r2_list     = []\n",
    "    mse_list    = []\n",
    "    rmse_list   = []\n",
    "    \n",
    "    # 선형회귀\n",
    "    # 트리모델\n",
    "    # 부스팅 기법 (XGB,LGB)\n",
    "    reg_classifiers = [\n",
    "        LinearRegression(),\n",
    "        RandomForestRegressor(),\n",
    "        XGBRegressor(),\n",
    "        LGBMRegressor()\n",
    "        ]\n",
    "\n",
    "    for item in reg_classifiers:\n",
    "        clf = item\n",
    "        clf.fit(np.array(X_train), np.array(Y_train))\n",
    "        y_pred = clf.predict(np.array(X_test))\n",
    "        y_true = np.array(Y_test)\n",
    "        r2,rmse,mse = evaludation(y_true,y_pred)\n",
    "        clf_list.append(str(clf).split('(')[0])\n",
    "        cat_list.append(cat)\n",
    "        feature_len.append(len(X_train.columns))\n",
    "        feature.append(str(X_train.columns.to_list()))\n",
    "        x_train_len.append(len(X_train))\n",
    "        x_test_len.append(len(X_test))\n",
    "        y_train_len.append(len(Y_train))\n",
    "        y_test_len.append(len(Y_test))\n",
    "        r2_list.append(r2)\n",
    "        mse_list.append(mse)\n",
    "        rmse_list.append(rmse)\n",
    "    eval_df = pd.DataFrame(list(zip(clf_list, cat_list,feature_len,feature,x_train_len,x_test_len,y_train_len,y_test_len,r2_list,mse_list,rmse_list)), \\\n",
    "               columns =['clf','cat', 'feature_len','feature', 'x_train_len','x_test_len', 'y_train_len','y_test_len', 'r2','mse','rmse']) \n",
    "    return eval_df\n",
    "\n",
    "before_eval_df =clf_grid('before feature selection',ori_X_train,ori_X_test,ori_Y_train,ori_Y_test) \n",
    "before_eval_df = before_eval_df.sort_values(by=['mse'])\n",
    "after_eval_df = clf_grid('After feature selection',X_train,X_test,Y_train,Y_test)\n",
    "after_eval_df = after_eval_df.sort_values(by=['mse'])\n",
    "\n",
    "print('============ Feature 선정/scaling 전 성능평가 ==========')\n",
    "print(before_eval_df[['clf','r2','mse','rmse']])\n",
    "print('============ Feature 선정/scaling 후 성능평가 ==========')\n",
    "print(after_eval_df[['clf','r2','mse','rmse']])\n",
    "\n",
    "print('================== Best Estimator ==================')\n",
    "print(after_eval_df.iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 결과해석 ( Explainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 의 Best Estimator를 기준으로 한다.\n",
    "#### Explainer모델로 가장 많이사용하는 lime을 통해 예측값에 대한 변수의 영향도는 어떤지 파악한다.\n",
    "#### - lime은 개별 관측치를 대상으로 설명하기에 1개의 관측치를 예시로 들어 볼 수 있으며,\n",
    "\n",
    "#### 관측치의 변수중 어떤변수가 어떤 방향으로(양/음) 영향도를 갖게되었는지 파악할 수 있다\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "clf = LGBMRegressor()\n",
    "clf.fit(np.array(X_train), np.array(Y_train))\n",
    "y_pred = clf.predict(np.array(X_test))\n",
    "\n",
    "explainer = LimeTabularExplainer(X_train.values,\n",
    "mode='regression',feature_names=X_train.columns)\n",
    "\n",
    "\n",
    "# asking for explanation for LIME model\n",
    "i = 10\n",
    "x_obs = X_test.iloc[[i],:]\n",
    "y_obs = Y_test.iloc[i]\n",
    "y_obs_pred = clf.predict(np.array(X_test.iloc[[i],:]))\n",
    "print('x_관측치 : \\n',x_obs)\n",
    "print('\\ny_관측치 : ',y_obs)\n",
    "print('y_관측치 대한 예측값 : ',round(float(y_obs_pred),2))\n",
    "explanation = explainer.explain_instance(x_obs.values[0],clf.predict)\n",
    "print(explanation)\n",
    "\n",
    "explanation.show_in_notebook(show_table=True, show_all=True)\n",
    "\n",
    "\n",
    "print( 'R2',explanation.score)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
